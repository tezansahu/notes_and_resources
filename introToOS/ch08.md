# Synchronisation Constructs

-   Limitations of mutexes & condition variables:
    -   Error prone [unlock/signal wrong mutex]
    -   Lack of expressive power [need helper variables for priority control, etc]

-   Synchronisation requires low-level hardware support [hardware atomic instructions]

-   __Spinlock:__
    -   Like a mutex: mutual exclusion; lock & unlock (free)
    -   But, if spinlock is busy, thread is not put to wait. It keeps spinning & checking repeatedly (burns CPU cycles) if the lock is free for acquiring
    -   Basic synchronisation construct

-   __Semaphores:__
    -   Like a mutex but more general
    -   It is an integer value [count-based synchronisation]
        -   On init, assigned a max value (positive int)
        -   On try (wait), 
            -   If non-zero, decrement & proceed [counting semaphore]
            -   If zero, wait
        -   On exit (post), increment
    -   Max. no. of threads allowed to execute a CS = max value of semaphore
    -   If initialized with 1 => binary semaphore
    -   Wait => `P()` [proberen => test out, try]
    -   Post => `V()` [verhogen => increase]
    -   To use semaphores in C, C++, 
        ```c
        #include <semaphore.h>
        sem_t <name>
        sem_init(&<name>, <shared (int)>, <max-val (int)>)
        sem_wait(&<name>)
        sem_post(&<name>)
        ```

-   __Reader/Writer Locks:__
    -   Specify the access [shared access for read & exclusive access for write] & then the lock behaves accordingly
    -   In C, C++:
        ```c
        #include <linux/spinlock.h>
        rwlock_t <name>

        read_lock(<name>)
        ... read-CS ...
        read_unlock(<name>)

        write_lock(<name>)
        ... write-CS ...
        write_unlock(<name>)
        ```
    -   Rwlocks are supported in Java, .NET, POSIX, but differ in implementation/semantics:
        -   What happens on read-unlock in case of recursive read-locks => some implementations require only 1 unlock for all locks, while others require individual unlocks
        -   In some implementations, read-locks are allowed to upgrade their priority to a write-lock when required. In other cases, the tread must release its readlock & then acquire a writelock for the same
        -   Interaction with underlying scheduling policies

-   __Monitors:__
    -   Higher-level synchronisation construct
    -   They specify:
        -   Shared resource
        -   Entry procedures [if something specific like readers/writers]
        -   Possible condition variables
    -   On entry => lock, check, etc
    -   On exit => unlock, check, signal, etc
-   Other synchronisation constructs:
    -   __Serializers:__ Make it easier to define priorities & hide the need to explicit use of signalling
    -   __Path expressions:__ Programmer specifies a regular expression indicating the type of executions [eg - multiple read, single write]
    -   __Barrier:__ Like a reverse semaphore [it will block execution until N threads arrive at that point]
    -   __Rendezvous point:__ Allows multiple threads to reach that point before continuing execution

-   Need for hardware support: Purely software implementations of spinocks need multiple CPU cycles for checking & setting the lock values & concurrent check/update on different CPUs may overlap. Hence we need some hardware support to guarantee atomic instructions

-   Hardware-specific atomic instructions:
    -   `test_and_set`
    -   `read_and_increment`
    -   `compare_and_swap`

-   __Atomic instructions__ specify the critical section & all synchronisation is supported by the hardware

-   __Shared Memory Multiprocessors:__
    -   Shared memory components can be connected to the CPUs using __bus-based connection__ or __interconnect (I/C) - based connection__ [common in current systems]
    -   I/C based connection can have multiple memory references in flight. This is not possible in a shared bus based connection
    -   CPUs have caches to hide memory latency
    -   In SMPs, shared memory is even more of an issue due to memory contention from multiple CPUs

    -   Types of write (in presence of cache):
        -   __No-write:__ write goes directly to memory & any cached copy of the memory location will be invalidated
        -   __Write-through:__ CPU write is applied both to cache & memory location
        -   __Write-back:__ write can be applied to cache & actual update to memory location could be delayed

-   __Cash coherence:__
    -   __Write-Invalidate (WI):__ If 1 CPU changes the value of variable present in caches of multiple CPUs then hardware ensures that any other reference to that memory location in any other cache will be invalidated & memory will be updated using write-through/write-back
    -   __Write-Update (WU):__  If 1 CPU changes the value of variable present in caches of multiple CPUs then hardware ensures that any other reference to that memory location in any other cache will be updated

-   Atomic operations always bypass caches & go directly to the memory controller:
    -   Can be ordered & synchronized to prevent race conditions that arise if atomics accessed caches of respective CPUs directly
    -   Disadvantages: 
        -   Takes much longer [takes longer to access memory & there will also be contention]
        -   Generates coherence traffic regardless of whether a change is applied by the atomic operation or not, to guarantee correctness

-   Spinlock performance metrics:
    1.   Reduce latency [time to acquire a free lock] - ideally immediately to execute atomic
    2.   Reduce wait time [time to stop spinning & acquire a lock that has been freed] - ideally immediately
    3.   Reduce contention [bus/network I/C traffic] - ideally 0

-   But, 1st & 2nd conflict with 3rd because implementing any of 1st & 2nd will lead to contention 

-   __Test-and-Set Spinlocks:__
    -   Minimal latency [just atomic]
    -   Potentially minimum delay [spinning continuously on the atomic operation]
    -   Contention - every thread waiting will go on the shared bus or I/C connection to the memory on every spin, leading to contention
    -   Real problem - spinning of the atomic operation [will go to memory even if cache coherence is supported, hence not most efficient]

-   __Test-and-test-and-set spinlock:__
    -   Separate test & set operations as two predicates of the while condition
    -   Test to be done only on cached value of lock
    -   Only if cached value changes, first predicate becomes false & second predicate to set is evaluated
    -   Also called __spin-on-read__
    -   Makes no difference if there is no cache coherence
    -   Fine if write-update is supported
    -   Horrible situation if write-invalidate is supported => contention due to atomics + cache invalidation
    -   Contention complexity in WU scenario with N processors: O(N) [all processors see that a lock is released & try to acquire the lock; as many memory references as test_and_set operations, ie, one for each processor]
    -   Contention complexity in WI scenario with N processors: O(N2) [all cached values of lock will be invalidated after lock is released; the processors that see the new value of the lock try to do the test_and_set operation, which in turn would invalidate the caches of all other processors; hence O(N2)]

-   Spinlock _delay_ alternatives:
    -   Delay after every lock release:
        -   Reduces contention
        -   Latency is OK
        -   Delay gets worse [after lock release, if there is no contention, we still have to wait]
    -   Delay after every lock reference:
        -   Doesn't spin constantly
        -   Works on non cache coherent architectures
        -   Can hurt delay even more
    -   Picking a delay value:
        -   Static Delay
            -   Based on fixed value like CPU ID
            -   Simple approach
            -   Unnecessary delay under low contention
        -   Dynamic Delay
            -   Backoff based
            -   Random delay in a range that increases with 'perceived' load
            -   Perceived => number of failed test_and_set operations

-   __Queueing Lock:__
    -   Solves the issue that _every thread sees the lock is free at the same time_
    -   _Flags_ array containing `has-lock` or `must-wait` values
    -   Set unique `ticket` for each arriving thread
    -   `queue[ticket]` is private lock
    -   It assumes `read_and_increment` atomic on hardware
    -   O(N) size
    -   For optimized latency & usage of cache, each element of the array must be placed on a new cache line