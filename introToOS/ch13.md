# Distributed Shared Memory

- Peer DIstributed Applications
    - Each node _owns_ a portion of the state & provide service to other nodes for accessing its state
    - All nodes are __peers__
    - Eg: big data analytics, web searches, content sharing, distributed shared memory (DSM) 
    - There could be some designated nodes that perform overall management & configuration tasks for the entire system. Hence, it is __NOT__ Peer-to-Peer system.

- In a DSM, each node:
    - Owns some 'state' [memory]
    - Provides some service:
        - memory read/writes
        - consistency protocols

- DSM permits scaling beyond the single machine memory limits
- Overall memory access could become slower, but such access differences are hidden by making smart design decisions
- Commodity Interconnect Technologies support DSM using __RDMA (Remote Direct Memory Access)__ based interfaces to provide low latencies while accessing remote memory
- Hardware vs Software DSM:

| Hardware DSM | Software DSM |
|--|--|
| Relies on interconnect | Everything done by software |
| OS manages larger physical memory | Managed by OS or language runtime |
| __NICs (Network Interconnect Cards)__ translate remote memory accesses to messages. NICs involved in all aspects of memory management & supports atomics | No explicit requirement of NICs |
| Expensive | Relatively inexpensive |

- DSM Design:
    - __Sharing Granularity:__ 
        - Adopting a solution where every single cache-line sized write message is sent to nodes across a network will potentially be very expensive
        - Alternatives:
            - __Variable granularity:__
                - Programmer can provide explicit support to tell DSM system how & when individual variables are to be shared
                - Potentially too fine granularity - may still have large overheads
            - __Page granularity:__
                - Implemented at OS level
                OS tracks when pages are modified & would trigger necessary messages to be exchanged with remote nodes for page modification
                - Possible to amortize the cost of remote access due to larger size of pages (~ 4kB)
            - __Object granularity:__
                - Requires some help from complier to lay application level objects on different pages
                - We can then rely on page-based OS level mechanism 
                - Alternative: DSM solution is supported by programming language & runtime, where runtime understands which objects are local vs remote, & the runtime generates all necessary communications with remote nodes only for the remote objects. Here, OS need not know about the DSM solution
        - __False Sharing:__ Consider a page/higher-level object to internally have 2 variables _x_ & _y_. A process on one node is exclusively accessing & modifying _x_ & does not care about _y_, whereas a process on another node is exclusively concerned with _y_. When _x_ & _y_ are shared on the same page & coarser granularity mechanisms are used, these writes would be interpreted as concurrent accesses to the same location & would trigger necessary coherence operations, invalidations, etc. But such overheads won't benefit any node
        - This can be avoided if programmer is careful how these variables are laid out on pages or higher-level objects
        - Or, we could rely on a compiler that actually understands what is a shared state & the allocates relevant variables onto a page
    
    - __Access Algorithms:__
        - __Single Reader / Single Writer (SRSW):__ 
            - Main role of DSM layer is to provide additional memory to provide the application with ability to access remote memory.
            - No sharing related challenges to be supported
        - __Multiple Reader / Single Writer (MRSW)__ 
        - __Multiple Reader / Multiple Writer (MRMW)__ 
        - For the last 2, DSM has to ensure that:
            - The _read_ returns correctly written most recent value of memory location
            - All writes are correctly ordered
    
    - __Performance Considerations:__
        - Major performance metric -> __access latency__
        - __Migration:__
            - When a process in another node wants to access remote state, the state is copied over to the other node
            - Makes sense for SRSW situation (only 1 node at a time will be accessing the state)
            - Requires data movement, which requires overheads
        - __Replication:__
            - More general 
            - State is copied on multiple nodes (potentially all nodes)
            - Similar to behavior of caching
            - Needs consistency management
            - Overheads can be limited by limiting the number of replicas that exist in a system at a point of time
    - __Consistency Management:__
        - Behavior of DSM ~ shared memory on SMPs? 
            - Coherence operations are triggered on each _write_ 
            - Overheads too high
        - Behavior of DSM ~ shared memory on DFSs? (more relevant)
            - Push invalidations when data is written to (proactive/eager/pessimistic)
            - Pull modifications periodically on demand (reactive/lazy/optimistic)

- DSM Architecture (Page-Based, OS Supported):
    - Assume only a portion of the physical memory of each node is contributed for the DSM service & can be explicitly addressed
    - The pool of memory regions that each of these nodes contributes forms the __global shared memory__
    - Every address in this pool is uniquely identified based on:
        - Identifier for the node on which it is residing
        - Page frame number of particular physical memory location
    - The node where a page resides is called __home node__ of that page
    - If MRMW:
        - Need local caching for performance (latency)
        - Home node drives all coherence operations
        - All nodes are responsible for some portion of the DSM operations
    - Home node keeps track of:
        - pages accessed
        - modifications
        - caching enabled/disabled
        - page locked?
    - When a page is particularly accessed by a node that is not the home node, it is too expensive to contact the home node to perform updates
    - This is prevented using notion of __current owner__ (not same as home node) - exclusive writer for a page & can drive all consistency related operations
    - Owners may change & home node keeps track of owner at a point of time
    - Explicit page replica can be created for load balancing, hot spot avoidance, reliability, etc.
    - Home node manages these replicas

- DSM Metadata:
    - Each page (object) has :
        - address [Node ID + Page Frame no.]
        - node ID [Home node]
    - Global map:
        - object (page) id => manager node id
        - manager map is available on every node
        - Global map must be replicated on every node
    - Metadata for local pages:
        - Per-page metadata is distributed across managers
        - Metadata for a page (object) is _partitioned_ across all of its home nodes
    - All this assumes 1 manager per page (object). To relax this:
        - We can take object id & use it as an index to a __mapping table__ (used in every node for the global map, & every entry here will indicate a manager node)
        - In case manager needs to change, we need to change the entry only in the mapping table. The object id remains same

- Implementing DSMs:
    - DSM must intercept all accesses to shared state:
        - to send remote messages requesting access
        - to trigger coherence messages
    - These overheads should be avoided while accessing non-shared pages
    - Dynamically _engage_ & _disengage_ DSM when required
    - For this, DSM implementation can leverage _Hardware MMU support_
        - Remote address memory => Trap & pass to DSM to send message
        - Cached content => Trap & pass to DSM to perform necessary coherence operations
    - For object-based DSMs, we can leverage the compiler to generate code to perform these checks at software level

- __Consistency Models:__
    - Agreement between memory (state) & upper software levels that the memory behaves correctly if & only if the software allows specific rules
    - Memory layer may expose certain APIs (locks, etc) which the software can use to achieve stronger guarantees
    - __Strict Consistency:__ 
        - Updates visible everywhere immediately
        - All nodes see writes that have happened in the system in the exact same order
        - In practice, even in SMPs, no guarantee is given on order without extra locking, etc.
        - In DSM systems, latency, message loss/reorder make it _impossible_ to guarantee this consistency
    - __Sequential Consistency:__
        - As long a the ordering is equivalent to some possible ordering of the operations if they executed on a single SMP, it is considered a legal order
        - Memory updates from different processors may be arbitrarily interleaved
        - All processors will see the exact same interleaving 
        - Operations from the same process always appear in order they were issued
    - __Causal Consistency:__
        - They will detect possible causal relationship between updates
        - If updates are causally related, then memory guarantees that those operations would be correctly ordered
        - For writes that are not causally related, there are no guarantees
        - Operations from the same process always appear in order they were issued
    - __Weak Consistency:__
        - __Synchronization Points:__ 
            - Operations that the memory system makes available to upper layers (R, W, Sync)
            - All updates prior to a sync will be visible
            - No guarantees on what happens in between
        - _Sync_ should be called by both processes (one that performs the updates & one that wishes to see the updates)
        - Variations:
            - Single sync operation (_sync_)
            - Separate sync operations per subset of state or pages
            - Separate _entry/acquire_ vs _exit/release_ operations
        - Limit the data movement & coherence operations
        - Maintain extra state for additional operations
