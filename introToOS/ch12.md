# Distributed File Systems

- Multiple machines involved in delivery of the file system service

- DFS Models:
    - Client-Server on different machines
    - File server distributed on multiple machines:
        - __Replicated:__ All files _replicated_ on all machines
        - __Partitioned:__ Each server contain different files. More scalable
        - Both: Partition all files & each partition replicated
    - File stored on & served from each machine
        -Blurred distinction b/w client & server

- Remote File Service:
    - Extreme 1: Upload/Download
        - File moved to client from server
        - Accesses are done on client 
        - File is returned to server
        - More like FTP, SVN _(Apache Subversion)_
        - Local read/write at client
        - Entire file needs to be downloaded/uploaded even for small accesses
        - Server has to give up file control
    - Extreme 2: True Remote File Access
        - File stays on server
        - Every access to remote file goes through server
        - File access centralized & easy to reason about consistency
        - Every operation pays network cost
        - Limits server scalability
    - More Practical Remote File Access (with Caching)
        - Allow clients to store parts of file locally _(blocks)_
            - Low latency on file operations
            - Server load reduced => more scalable
        - Force clients to interact with servers frequently
            - Server has insights into what clients are doing
            - Server has control over which accesses van be permitted or not => easier for consistency
        - However, server becomes more complex
    
- Stateless vs Stateful File Server

|Stateless|Stateful|
|--|--|
| Keeps no state | Keeps client state |
| OK with extreme models, but can't support 'practical' model | Needed for 'practical' model to track what is cached/accessed |
| Cannot support caching & consistency management | Can support locking, caching & incremental operations |
| On failure, just restart | On failure, need checkpointing & recovery mechanisms |
| No resources used on server side (CPU/memory) | Overheads to maintain state & consistency |
| Every request is self contained => more bytes transferred | |

- Caching state in  DFS:
    - Locally client maintains a portion of the state
    - Locally client performs operations on cached state
    - Requires coherence mechanisms
    - In DFS, it is client/server-driven & triggered on demand, periodically or on file open
    - Details depend on file sharing semantics
    - Files/File Blocks can be cached in:
        - client memory
        - client storage device
        - buffer cache in memory on server (usefulness would depend on client load, request interleaving, etc.)

- File sharing semantics in DFS:
    - __UNIX semantics:__ Every write is visible immediately (like local file system)
    - __Session Semantics:__ 
        - Write back on `close()` & update on `open()`
        - Period b/w `open()` & `close()` is a __session__
        - Easy to reason, but may be insufficient
        - Eg: For server-driven DFS with session semantics, server maintains following data in per-file data structure:
            - readers
            - current writers
            - file version
    - __Periodic Updates:__
        - Clients write back periodically => they have some kind of _lease_ on cached files
        - Server invalidates periodically => provides bounds on inconsistency
        - Augment with `flush()` [to flush updates to remote server] & `sync()` [sync state with server] APIs
    - __Immutable Files:__ Never modify; New files are created on update [Eg: sharing photos on FB, Instagram]
    - __Transactions:__ All changes are atomic

- Regular files & directories have very different access patterns. Hence we could apply different semantics for either type of files.

- Replication & Partitioning can be combined as follows:
    - Files are partitioned across different groups or in different volumes & each of these groups is replicated, potentially with a different degree of replication
    - Eg: Partitions of read-only files vs file that are written to, & replicate read-only files to greater degree
    - Eg: Have smaller partitions with more frequently accessed files vs larger partitions of more files that are less frequently accessed & replicate so that overall each machine has almost same number of client requests

- __Network File System (NFS):__
    - Clients request & access files using the VFS interface & same types of file descriptors used to access local files
    - VFS layer determines if file belongs to local FS or if it needs to be pushed to NFS client
    - NFS client interacts via RPC with NFS server on remote machine that actually stores files
    - NFS server accepts requests, forms them into appropriate file system operations & issue them to local VFS
    - On `open()` request, a file _handle_ gets created on server & returned back to client, where it is retained by NFS client to be used on subsequent requests involving same file
    - NFSv3 - stateless
    - NFSv4 - stateful
    - Caching semantics:
        - Session-based (non-concurrent)
        - Periodic updates:
            - Default: 3s for regular files, 30 s for directories
        - NFSv4 uses _delegation to client for a period of time_ (all rights to manage a file are delegated to a client for a period of time)
    - Locking semantics:
        - Lease-based locks acquired by clients for a time period
        - NFSv4 supports _share reservation_ using reader/writer locks

- __Sprite DFS:__
    - Analysis of File Access:
        - 33% of file accesses are  writes
        - 75% of files are open for < 0.5 s
        - 90% of flies are open for < 10 s
        - 20-30% new data is deleted within 30 s
        - 50% new data is deleted within 5 min
        - File sharing is _rare_
    - Design of DFS:
        - Cache with write-back: 
            - Every 30 s, write-back blocks that have not been modified for last 30 s
            - When another client opens file, get _dirty_ blocks
        - Every `open()`goes to server -> directories can't be cached
        - On concurrent writes, disable caching
    - File access operations in Sprite:
        - Assume R<sub>1</sub> - R<sub>n</sub> readers & W<sub>1</sub> writer
        - All `open()` go through server
        - All clients cache bocks
        - Writer keeps timestamps for each modified bock
        - Client maintains the following per file:
            - in cache (y/n)
            - cached blocks
            - timer for each _dirty_ block
            - version
        - Server maintains the following per file:
            - readers
            - writer
            - version
            - cacheable (y/n)
        - If W<sub>2</sub> is a __sequential writer__, 
            - Server contacts last writer for dirty blocks
            - If W<sub>1</sub> has closed, update version
            - W<sub>2</sub> can now cache file
        - If W<sub>3</sub> is a __concurrent writer__,
            - Server contacts last writer for dirty blocks
            - Since W<sub>2</sub> hasn't closed, disable caching
    
