# Inter-Process Communication

-   __IPC:__ OS supported mechanisms for interaction among processes (coordination & communication)

-   Message-Passing Based: Eg - sockets, pipes, message queues

-   Memory-based: Eg - shared memory, memory mapped file

-   __Message-Passing IPC:__
    -   `send`/`recv` messages
    -   OS creates & maintains a channel - buffer, FIFO queue
    -   OS provides interface to processes - a __port__
    -   Processes send/write to the port or recv/read from the port
    -   Kernel is required to :
        -   Establish communication
        -   Perform each IPC operation
    -   Thus, send/recv requires system call + data copy
    -   Request-Response needs 4x user-kernel crossings + 4x data copies [overheads]
    -   __Pipes:__ Carry byte stream between 2 processes [connect o/p of one process to i/p of another process]
    -   __Message Queues: __
        -   Carry _messages_ among processes
        -   OS management includes priorities, scheduling of msg delivery
        -   APIs - SysV & POSIX
    -   __Sockets:__
        -   `send()`, `recv()` == msg passing buffers
        -   `socket()` == create kernel-level socket buffer
        -   Associate necessary kernel-level processing
        -   If different  machines, channel b/w process & network device

        -   If same machine, bypass full protocol stack

-   __Shared-Memory IPC:__
    -   Read & write to shared memory location
    -   OS establishes a shared channel b/w processes
    -   Physical pages are mapped to virtual address spaces of each process
    -   VA(P<sub>i</sub>) & VA(P<sub>j</sub>) map to the same physical address while VA(P<sub>i</sub>) != VA(P<sub>j</sub>)
    -   Physical memory need not be contiguous
    -   Advantages:
        -   System calls only for setup
        -   Data copies potentially avoided (not eliminated)
    -   Disadvantages:
        -   Explicitly synchronized
        -   Shared memory management to be done by programmer

-   For Message Passing, CPU cycles are used to copy data from/to ports 

-   For Shared Memory IPC, CPU cycles are used to map memory to address space & copy data to channel & is useful in cases where: 
    -   Setup once, use many times
    -   1-time use for large  data where T<sub>copy</sub> >> T<sub>map</sub>

-   Windows uses __"Local" Procedure Calls (LPC)__ when T<sub>copy</sub> is less than a threshold

-   __System V Shared Memory API:__ [[documentation](http://www.tldp.org/LDP/lpg/node21.html)]
    -   _Segments_ of shared memory - need not be contiguous physical pages
    -   Shared memory os system-wide => system limits on number of segments and total size
    -   Create: 
        -   OS assigns a unique key
        -   `shmget(shmid, size, flag)` [create or open]
        -   `ftok(pathname, proj-id)` [to generate unique key; similar to hash function]
    -   Attach: 
        -   Map VA->PA
        -   `shmat(shmid, addr, flag)` 
    -   Detach: 
        -   Invalidate address mapping [can be reattached]
        -   `shmdt(shmid)`
    -   Destroy: 
        -   Only remove when explicitly deleted [this is different from usual memory & malloc]
        -   `shmctl(shmid, cmd, buf)` [Destroy with IPC_RMID]

-   POSIX Shared Memory API:
    -   Uses files instead of segments
    -   `shm_open()`
        -   Returns file descriptor [similar to key in SysV]
        -   In tmpfs [not an actual file system]
    -   `mmap()` & `unmmap()
        -   Mapping VA -> PA
    -   `close()`
        -   Close the file descriptor allocated by shm_open() when it is no longer needed
    -   `shm_unlink()`
        -   Remove a shared memory object name

-   IPC synchronization:
    -   Mechanisms supported by process threading library
    -   OS supported IPC for synchronization
    -   Either method must coordinate:
        -   Number of concurrent accesses to shared memory region
        -   When data is available & ready for consumption by peers

-   Shared Memory Design Considerations:
    -   How many segments?
        -   1 large segment => manager for allocating/freeing memory from shared segment
        -   Many small segments => use pool/queue of segments & communicate the segment IDs with processes using other channels like message queues
    -   Size of segments?
        -   Works for well-known static sizes, but there are limits on max data size
        -   If segment size < message size, transfer data in rounds & include protocol to track progress (such as extra flags as parameters)