# Scheduling

-   A CPU Scheduler:

    -   Chooses one of the ready tasks to run on CPU
    -   Runs when:
        -   CPU becomes idle
        -   New task is available
        -   Timeslice expires

-   __Runqueue:__ DS used to represent a ready queue

-   Runqueue is tightly coupled with scheduling algorithm

-   __"Run-to-completion" scheduling:__
    -   Initial Assumptions:
        -   Group of tasks/jobs
        -   Known execution times
        -   No preemptions
        -   Single CPU
    -   Metrics:
        -   Throughput
        -   Avg. job completion time
        -   Avg. job wait time
        -   CPU utilization
    -   __FCFS:__
        -   Schedule tasks in order of arrival
        -   Runqueue = queue (FIFO)
    -   __SJF:__
        -   Schedules tasks in order of execution time
        -   Runqueue = ordered queue / tree

-   __Preemptive Scheduling:__
    -   If using SJF, scheduler checks running time of incoming process & compares it to currently running process & preempts if necessary
    -   Execution time is not known in general => use heuristics based on history like:
        -   How long did the task run last time?
        -   How long did the task run last n times? [windowed avg.]
    -   __Priority Scheduling:__
        -   Tasks have different priority levels
        -   Run next highest priority task next [with preemption]
        -   We could have multiple runqueues, one for each priority level
        -   Low priority task can be starved => Can be fixed by  __priority aging__ [priority = f(actual priority, time spent in runqueue)]

-   __Priority Inversion:__ scenario in which a higher priority process is preempted by a lower priority process. Could be because a low priority thread has acquired a lock which is later needed by a high priority thread. Due to unavailability of lock, the high priority thread is kept in a wait queue & medium/low priority threads get chance to run & finish prior to high priority thread.

-   Solution for priority inversion => temporarily boost the priority of mutex owner thread

-   __Round Robin Scheduling:__
    -   Pick up first task from queue like FCFS
    -   Task may yield to wait for I/O [unlike FCFS]
    -   Variants:
        -   RR with priorities [needs preemption]
        -   RR with interleaving [timeslicing]

-   __Timeslice:__
    -   Max amt of uninterrupted time that could be given to a task [_time quantum_]
    -   Task may run less than timeslice time:
        -   Has to wait on I/O, synchronisation => will be placed on a queue
        -   Higher priority task becomes runnable
    -   Allows CPU timesharing
    -   CPU bound tasks - preempted by timeslicing
    -   Advantages:
        -   Short tasks finish sooner
        -   More responsive
        -   Lengthy I/O ops initiated sooner
    -   Disadvantages:
        -   Overheads => keep timeslice >> Tctx_switch 
    -   For CPU-bound tasks, responsiveness [avg. wait time] is not a very relevant metric. So, we try to choose larger timeslices to increase the throughput & avg. completion time. Best case scenario: timeslice -> infinity [but this will result in worst-case wait times]
    -   For I/O-bound tasks, value of timeslice is not really relevant but smaller timeslices are preferred [keeps CPU & device utilization high]

-   If we want CPU & I/O bound tasks to have different timeslice values, we could maintain:
    -   Same runqueue & check type
    -   Two different structures

-   Dealing with different timeslices:
    -   Maintain a runqueue as a multi-queue internally. The different queues correspond to:
        -   Most I/O intensive tasks
            -   Shortest timeslice
            -   Highest priority
        -   Medium I/O intensive tasks:
            -   Medium timeslice
            -   Mix of I/O & CPU processing
        -   CPU intensive tasks:
            -   Infinite timeslice [FCFS]
            -   Lowest priority
        -   Advantages:
            -   Timeslicing benefits provided for I/O intensive tasks
            -   Timeslicing overheads avoided for CPU-intensive tasks
        -   How to know if a task is I/O or CPU intensive? Also, how I/O intensive?
            -   History based heuristics [similar to SJF]
            -   What if new job or dynamic change of phase in behaviour?
                -   Treat the runqueue not as a multiqueue, but as a single queue externally
                -   Tasks enter the topmost queue
                -   If task yields voluntarily => good choice! Keep task at same level
                -   If task uses up entire timeslice =>push down to a lower level
                -   Task in lower queue gets "priority boosted" if it releases CPU due to I/O waits
                -   This DS is __Multi Level Feedback Queue [MLFQ]__
                -   Solaris scheduler is a MLFQ with 60 levels & some fancy feedback rules

-   __Linux O(1) scheduler:__
    -   Constant time to select/add  task, regardless of task count
    -   Preemptive & priority-based with 140 priority levels
    -   Levels 0-99: realtime tasks
    -   Levels 100-139: timesharing tasks
    -   User processes;
        -   Default 120
        -   Can be modified with a "nice value" [-20 to 19, to span the entire timesharing priority levels]
    -   Timeslice values:
        -   Depends on priority
        -   Smallest for low priority CPU-bound tasks
        -   Highest for high priority I/O bound tasks
    -   Feedback:
        -   Sleep time [wait/idle time]
        -   Longer sleep => interactive => priority - 5 [boost]
        -   Smaller sleep => CPU-intensive => priority + 5 [lowered]
    -   Runqueue:
        -   2 arrays of tasks:
        -   Active
            -   Used to pick next task to run
            -   Constant time add/select
            -   Tasks remain in queue in active array until timeslice expires
        -   Expired
            -   Inactive list
            -   When no more tasks in active array => swap active & expired arrays
        -   This mechanism explains why high priority tasks are given larger timeslices [to allow them to run longer until timeslice expires & they get placed in expired array]
        -   This also serves as a priority-aging mechanism to prevent low priority tasks from starvation
        -   Since low-priority tasks have shorter timeslices, they won't affect running of high priority tasks in expired array by much
    -   Introduced in Linux 2.5, but workloads changed & became more time-sensitive leading to more jitter caused by the O(1) scheduler
    -   Disadvantages:
        -   Performance of interactive tasks  affected [_jitter_]
        -   No fairness guarantees
    -   Replaced by __Completely Fair Scheduler [CFS]__ in Linux 2.6

-   __Linux CFS Scheduler:__
    -   Runqueue: [__Red-Black tree__](https://algs4.cs.princeton.edu/33balanced/) [belongs to a family of self-balancing trees]
    -   Ordered by __vruntime__ [time spent on CPU] - least vruntime tasks appear at the left nodes [& leaves do not play any role as they are NULL]
    -   CFS algorithm:
        -   Always pick leftmost node
        -   Periodically adjust vruntime
        -   Compare to leftmost vruntime
            -   If smaller => continue running
            -   If larger => preempt & place appropriately in the tree
    -   Vruntime  progress rate depends on priority & "niceness"
        -   Rate faster for low-priority tasks
        -   Rate slower for high-priority tasks
    -   Performance:
        -   Select task: O(1)
        -   Add task: O(log N)

-   Scheduling on Multi-CPU systems:
    -   Important to have cache-affinity 
        -   Keep a task on the same CPU as much as possible
        -   Hierarchical scheduler architecture with a load balancer  at top-level to divide tasks among the CPUs & a per-CPU scheduler with a per-CPU runqueue schedules tasks on the same CPU as much as possible
        -   Load balancing is done based on runqueue length or when CPU is idle
    -   __Non Uniform Memory Access [NUMA]:__
        -   There could be multiple Memory Nodes for each CPU
        -   Memory nodes closer to a "socket" of a multiprocessor
            -   Access to local memory node faster than access to  remote memory node (connected to other CPUs)
        -   Keep tasks on the same CPU close to the memory node containing its state [__NUMA-aware scheduling__]

-   __Hyperthreading:__
    -   Multiple hardware-supported execution contexts [multiple registers on same CPU to hold information about multiple threads]
    -   Still 1 CPU, but with very fast context switching
    -   Also known as __hardware multithreading__, __chip multithreading [CMT]__ or __simultaneous multithreading [SMT]__
    -   Generally support 2 hardware threads but some high-end servers could support 8 threads
    -   SMT T<sub>ctx_switch</sub> ~ cycles & memory load ~ 100 cycles => makes sense to context switch & also hide memory access latency

-   Scheduling for SMTs:
    -   Assumptions:
        -   Thread issues instructions on each cycle => Max __Instructions Per Cycle (IPC)__ = 1 [for CPU bound tasks]
        -   Memory access takes 4 cycles
        -   Hardware switching instantaneous
        -   SMT with 2 hardware threads
    -   If we co-schedule CPU-bound threads:
        -   Threads interfere & contend for the single CPU pipeline
        -   Performance degrades by 2x
        -   Memory Idle
    -   If we co-schedule 2 memory bound threads:
        -   CPU idle
    -   Ideal option: co-schedule  CPU & Memory bound threads
        -   Minimal degradation
        -   Avoid/limit contention for CPU pipeline
        -   CPU & memory get well utilized
    -   To decide between CPU/Memory bound task, we use historic information but we need some hardware-level information
    -   Hardware counters store information:
        -   L1, L2, LLC misses
        -   IPC
        -   Power & energy data
    -   Software interface & tools to access hardware counters:
        -   Oprofile [Linux]
        -   Linux `perf` tool
    -   From hardware counters, (g)estimate resource requirements of a thread & use it in scheduling
    -   Scheduler makes informed decisions:
        -   Multiple hardware counters
        -   Models with per-architecture thresholds
        -   Based on well-understood workloads
    -   Using __Cycles Per Instruction [CPI]__ as a metric for co-scheduling threads could help increase performance, but realistic workloads don't have a great difference in CPI values to utilize this effect
    -   __LLC (Last Level Cache)__ usage would be a better choice