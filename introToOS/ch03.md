# Threads & Concurrency

-   Threads are part of the same virtual address space, they share all code, data, files, but potentially execute different instructions, access different portions of address space, operate on different portions of the input, & differ in other ways

-   Each thread has different PC (program counter), stack, stack pointer, thread-specific registers

-   OS represents multithreaded processes with a complex PCB

-   Why are threads useful?
    -   Parallelization => speedup
    -   Specialization => hot cash
    -   Efficiency => lower memory management & cheaper IPC

-   On 1 CPU, if T<sub>idle</sub> > 2*T<sub>ctx_switch</sub>, then context switch to hide idling time. T<sub>ctx_sw_thread</sub> < T<sub>ctx_sw_process</sub>. Hence, using threads hides latency associated with I/O operations

-   Multithreaded OS kernel:
    -   Threads working on behalf of apps
    -   OS-level services like daemons or drivers

-   What do we need to support threads?
    -   Thread DS: Identify threads, keep track of resource usage
    -   Mechanisms to create & manage threads
    -   Mechanisms to safely coordinate among threads running concurrently in same address space

-   Concurrency Control & Coordination:
    -   Mutual exclusion:
        -   Exclusive access to only one thread at a time
        -   Mutex
    -   Waiting on other threads
        -   Specific condition before proceeding
        -   Condition variable
    -   Waking up other threads from wait state

-   Thread type (DS): 
    -   Thread ID, PC, SP, registers, stack, attributes

-   `fork(proc, args)`:
    -   Creates a thread
    -   Not UNIX fork
    -   PC = proc
    -   Stack = args

-   `join(thread)`:
    -   Terminate a thread

-   __Mutex:__
    -   like a lock that should be used whenever accessing data or state shared among threads 
    -   Term used: _acquire_ the mutex/lock
    -   Unsuccessful threads will be blocked on the lock operation (suspended & won't be able to proceed until murex/lock is "released")
    -   Mutex struct holds: locked/unlocked status, owner, blocked threads
    -   __Critical section:__ Portion of code protected by mutex

-   __Condition Variable API:__
    -   Condition type (DS contains mutex ref, waiting threads, etc)
    -   `wait(mutex, cond)`:
        -   Mutex is automatically released & reacquired on wait
    -   `signal(cond)`:
        -   Notify only one thread waiting on condition
    -   `broadcast(cond)`:
        -   Notify all waiting threads

-   Use `while` instead of `if` to `wait()` on a condition variable:
    -   `While` can support multiple consumer threads
    -   The shared variable state can change before consumer gets access again
    -   Cannot guarantee access to mutex once condition is signaled

-   While using condition variables along with mutexes, the actual critical section is enclosed within "Enter critical section" & "exit critical section" blocks

-   Avoiding common mistakes:
    -   Keep track of mutex/cond. Variables used with a resource
    -   Check you are always & correctly using lock & unlock
    -   Use single mutex to access a single resource
    -   Check you are signalling correct condition
    -   Check you are not using signal when broadcast is needed (other way is OK, but degrades performance)

-   __Spurious wakeups:__
    -   When we wake-up threads knowing that they may not be able to proceed, hence wasting CPU cycles in context switching
    -   Can be prevented by unlocking the mutex before broadcast/signal (although not possible always)

-   __Deadlocks:__ 
    -   2 or more competing threads are waiting on each other to complete, but none of the ever do
    -   To avoid this, maintain a lock order (prevents cycles in the wait graph)
    -   A cycle in a wait graph in necessary & sufficient for deadlock to occur
    -   Edges of wait graph: from thread waiting on a resource to thread owning the resource
    -   Deadlock prevention (expensive)
    -   Deadlock detection & recovery (rollback)
    -   __Ostrich algorithm__, ie, do nothing!

-   Relationship b/w user-level & kernel-level threads:
    -   __One-to-one model:__
        -   OS sees/understands threads, synchronizations, blocking
        -   Must go to OS for all operations (expensive)
        -   OS may have limits on policies, thread number
        -   Portability issue
    -   __Many-to-one model:__
        -   Totally portable & does not depend on OS limits/policies
        -   OS has no insights into application level threads
        -   OS may block entire process if one ULT blocks I/O
    -   __Many-to-many model:__
        -   Best of both worlds
        -   Can have both bound/unbound threads
        -   Requires coordination between ULT & KLT thread managers

-   __System scope:__ system-wide thread management by OS-level thread managers

-   __Process Scope:__ user-level library manages threads within a single process

-   __Boss-Worker Pattern:__
    -   Boss assigns work to workers
    -   Worker performs entire task
    -   Throughput of the system is limited by boss thread => must keep boss efficient
    -   Boss assigns work by:
        -   Directly signalling specific worker
        -   Placing work in producer consumer queue => better
    -   Boss doesn't need to know about details of workers
    -   Queue synchronization is required
    -   Deciding number of workers:
        -   On demand
        -   Pool of workers => better (can be static or dynamic)
    -   Simple, but needs thread-poll management
    -   Can't take advantage of locality of threads which could be used for optimization
    -   Boss worker variants:
        -   All workers created equal (discussed previously)
        -   Workers specialized for certain tasks
            -   Better locality
            -   QoS management
            -   Load-balancing may be an issue
    -   ![](https://latex.codecogs.com/png.latex?Boss-worker\ Formula:Tot\ time&space;=&space;timeToFinish1Order\ \times\ ceil(\frac{numOrders}{numConcurrentThreads}))

-   __Pipeline Pattern:__
    -   Threads assigned one subtask in the system
    -   Entire task = pipeline of threads
    -   Multiple threads concurrently in the system, in different pipeline stages
    -   Throughput == weakest link
    -   Pipeline stage can be a thread-pool
    -   Shared buffer based communication b/w stages
    -   Specialization & locality used effectively
    -   Balancing & synchronization overheads are an issue
    -   ![](https://latex.codecogs.com/png.latex?Pipeline&space;\ &space;Formula:&space;Tot\ time&space;=&space;timeToFinish1Order\ &plus;\ (remainingOrders\ \times\ timeToFinishLastStage))

-   __Layered Pattern:__
    -   Each layer group of related subtasks
    -   End-to-end task must pass up & down through all layers
    -   Specialization
    -   Less fine-grained than pipeline
    -   Not suitable for all applications
    -   Synchronization issues

-   [Pthreads tutorial](https://computing.llnl.gov/tutorials/pthreads/)